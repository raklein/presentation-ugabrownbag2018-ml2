<!DOCTYPE html>
<html>
  <head>
    <title>Many Labs 2</title>
    <meta charset="utf-8">
    <meta name="author" content="Richard Klein  LIP/PC2S   Université Grenoble Alpes" />
    <link href="libs/remark-css/tamu-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/tamu.css" rel="stylesheet" />
    <link href="libs/remark-css/lucy-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Many Labs 2
## Investigating Variation in Replicability across Sample and Setting<br><br>
### Richard Klein<br /> LIP/PC2S <br /> Université Grenoble Alpes
### 2018-12-10 (updated: 2018-12-12)

---




class: center, middle

# Many Labs 2

---


# Cause for Concern

--

.center[&lt;img src = "images/concern1.png" width = "500"/&gt;]
--


.center[&lt;img src = "images/bem.png" width = "500"/&gt;]
--


.center[&lt;img src = "images/concern2.png" width = "500"/&gt;]

---

# Flexibility in Data Analysis

.center[http://fivethirtyeight.com/features/science-isnt-broken
]

.center[&lt;img src = "images/538_neutral.png" width = "550"/&gt;]

---

# Flexibility in Data Analysis

.center[http://fivethirtyeight.com/features/science-isnt-broken
]

.center[&lt;img src = "images/538_dem.png" width = "550"/&gt;]

---

# Flexibility in Data Analysis

.center[http://fivethirtyeight.com/features/science-isnt-broken
]

.center[&lt;img src = "images/538_rep.png" width = "550"/&gt;]

---

# Failures to Replicate

--

- `Reproducibility Project: Psychology` &lt;small&gt;(OSC, 2015)&lt;/small&gt;  
  - ~40/100 studies replicated
--

- `Social Sciences Replication Project` &lt;small&gt;(Camerer et al., 2018)&lt;/small&gt;
  - 13/21 replicated
  - All from Science and Nature
--

- `Multiple large-scale Registered Reports` 
  - POPS/AMPPS Registered Replication Reports

---

# Failures to Replicate

--

- What we know: Many studies are failing to replicate
--

- Why? Not sure
--

  - Could be false positives
--

  - Could be many other reasons: 
  - Moderators (known/unknown)
  - Lack of care/expertise
  - Sensitivity of effects to sample/context

---


# Many Labs Projects

--

Each ML project examines a different aspect of replication. Each question requires data colletion at multiple labs.  
- `Many Labs 1` &lt;small&gt;(Klein et al., 2014)&lt;/small&gt;  
  - 10/13 successful replications
  - Little variation between samples
--

- `Many Labs 2` &lt;small&gt;(Klein et al., in press)&lt;/small&gt;
  - Discussing today
--

- `Many Labs 3` &lt;small&gt;(Ebersole et al., 2016)&lt;/small&gt;
  - 3/10 successful replications
  - Little variation across semester
--

- `Many Labs 4` &lt;small&gt;(Klein et al., in prep)&lt;/small&gt;
  - Terror Management Theory-specific
  - Compare expert replications vs "in-house" replications
--

- `Many Labs 5` &lt;small&gt;(Ebersole et al., in prep)&lt;/small&gt;
  - Follow-up to Reproducibility Project
  
---

# Many Labs 2

--

Like Many Labs 1, but a much stronger test:
--

- Goal: Replicate many different studies all around the world and compare if they vary based on the sample of data collection.
--

- Replicated 28 studies
  - Split across two study "packages" due to length
  - Computerized in Qualtrics
  - Randomized study order, presented back-to-back
--
  
- Which studies?
  - Structured selection process by committee. Documented: osf.io/8cd4r
  - Sought open nominations for studies
  - Emphasized impact (citations, etc.), diversity of content, possibility for variability across sites
  - But substantial practical constraints: Short, able to be computerized
  - Authors could decline to be replicated

---

# Many Labs 2

--
- Registered Replication Report at AMPPS:
  - Each study reviewed and approved by original authors or other experts
  - Analysis plan(s) specified in advance (osf.io/c97pd/)
  - Open data and materials

--
- Administer packages across 125 samples
  - Slate 1: 13 studies administered in each of 61 labs
  - Slate 2: 15 studies administered in each of 64 labs
  - Sites (mostly) randomly assigned to slates
  - Minimum of 80 participants per site
  - 15,305 participants total
  - Much more diverse

---

# Many Labs 1 Map

.center[&lt;img src = "images/ml1map.png" width = "750"/&gt;]

---

# Many Labs 2 Map

.center[&lt;img src = "images/ml2map.png" width = "750"/&gt;]

---

# Many Labs 2 Hsee example

.center[&lt;img src = "images/hsee_example.png" width = "750"/&gt;]

---

# Many Labs 2 Hsee results

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;

.center[&lt;img src = "images/hsee.png" width = "750"/&gt;]

---

.center[&lt;img src = "images/fig2.png" width = "610"/&gt;]

---

# Many Labs 2 Results

--
- 14/28 successful replications
  - &lt;i&gt;p&lt;/i&gt; &lt; .0001, non-trivial effect size, same direction as original
  - One weakly supported, &lt;i&gt;p&lt;/i&gt; = .03 but near-zero effect size

--
- 75% has smaller effect size than original
  - Median original &lt;i&gt;d&lt;/i&gt; = 0.60
  - Median replication &lt;i&gt;d&lt;/i&gt; = 0.15

--
- No evidence that the order of the studies mattered
  - In general, didn't matter if the study came first, last, or in any other position.
  - Same as ML1 and ML3

--

.center[&lt;img src = "images/fig4_hsee.png" width = "700"/&gt;]

---

.center[&lt;img src = "images/fig4.png" width = "700"/&gt;]

---

# Many Labs 2 Heterogeneity

--
- Q statistic: (~ significance test for variation across sites exceeding chance)
  - 11/28 (39%) showed significant heterogeneity
  - Nearly all from larger-effects studies

--
- I²:
  - 36% showed at least medium heterogeneity
  - Likely not an appropriate measure in this case:
  - See [osf.io/frbuv](osf.io/frbuv) (Marcel van Assen), [Datacolada.org/63](Datacolada.org/63), Borenstein+Higgins

--
- Tau is probably best
  - SD across samples in the unit of the effect size (after accounting for sampling error)

---

.center[&lt;img src = "images/table3.png" width = "800"/&gt;]

---


.center[&lt;img src = "images/table3_q.png" width = "800"/&gt;]

---


.center[&lt;img src = "images/table3_tau.png" width = "800"/&gt;]

---

# Discussion

--
- Low variation across sample/context

--
  - Not reasonable to discount replications &lt;i&gt;by default&lt;/i&gt; based on sample

--
  - Instead, test moderators empirically

--
- Big caveat: Mostly student samples, mostly short computerized studies

--
- Replication rate aligns with other projects
  - Is this meaningful?

--
- Many studies replicate robustly

--
- Personal takeaways:

--
  - Robust replicability is a feasible goal (for many studies)

--
  - Nudges me towards "false-positive" explanation for replication failures (in general)

--
  - Reinforces need for preregistration/Registered Reports

--
- Open data: [https://osf.io/8cd4r/](https://osf.io/8cd4r/)
  - CC0, free use (any purpose)
  - We barely scratched surface

---

class: center, middle

# Thanks!

Special thanks to co-leads Fred Hasselman, Michelangelo Vianello, and Brian Nosek + 186 other co-authors.
  
Questions/comments?

&lt;br&gt;
&lt;br&gt;
&lt;br&gt;
@raklein3 &lt;br&gt;
raklein22@gmail.com &lt;br&gt;
https://www.raklein.me 

&lt;br&gt;
&lt;br&gt;

&lt;img src = "images/ugalogo.png" height = "75"/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img src = "images/liplogo.png" height = "75"/&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp;&lt;img src = "images/idexlogo.png" height = "50"/&gt;
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"ratio": "4:3"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
